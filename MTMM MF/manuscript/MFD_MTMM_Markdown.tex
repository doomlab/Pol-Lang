\documentclass[english,man]{apa6}

\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{â‚¬}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}

% Table formatting
\usepackage{longtable, booktabs}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\begin{center}\begin{threeparttable}}
%   {\end{threeparttable}\end{center}\end{landscape}}

\newenvironment{lltable}
  {\begin{landscape}\begin{center}\begin{ThreePartTable}}
  {\end{ThreePartTable}\end{center}\end{landscape}}

  \usepackage{ifthen} % Only add declarations when endfloat package is loaded
  \ifthenelse{\equal{\string man}{\string man}}{%
   \DeclareDelayedFloatFlavor{ThreePartTable}{table} % Make endfloat play with longtable
   % \DeclareDelayedFloatFlavor{ltable}{table} % Make endfloat play with lscape
   \DeclareDelayedFloatFlavor{lltable}{table} % Make endfloat play with lscape & longtable
  }{}%



% The following enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand\getlongtablewidth{%
 \begingroup
  \ifcsname LT@\roman{LT@tables}\endcsname
  \global\longtablewidth=0pt
  \renewcommand\LT@entry[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}%
  \@nameuse{LT@\roman{LT@tables}}%
  \fi
\endgroup}


\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            pdfauthor={},
            pdftitle={A Validation of the Moral Foundations Questionnaire and Dictionary},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=black,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls

\setlength{\parindent}{0pt}
%\setlength{\parskip}{0pt plus 0pt minus 0pt}

\setlength{\emergencystretch}{3em}  % prevent overfull lines

\ifxetex
  \usepackage{polyglossia}
  \setmainlanguage{}
\else
  \usepackage[english]{babel}
\fi

% Manuscript styling
\captionsetup{font=singlespacing,justification=justified}
\usepackage{csquotes}
\usepackage{upgreek}

 % Line numbering
  \usepackage{lineno}
  \linenumbers


\usepackage{tikz} % Variable definition to generate author note

% fix for \tightlist problem in pandoc 1.14
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% Essential manuscript parts
  \title{A Validation of the Moral Foundations Questionnaire and Dictionary}

  \shorttitle{VALIDITY OF MFD}


  \author{Kayla N. Jordan\textsuperscript{1}, Erin M. Buchanan\textsuperscript{2}, \& William E. Padfield\textsuperscript{2}}

  % \def\affdep{{"", "", ""}}%
  % \def\affcity{{"", "", ""}}%

  \affiliation{
    \vspace{0.5cm}
          \textsuperscript{1} University of Texas - Austin\\
          \textsuperscript{2} Missouri State University  }

  \authornote{
    Kayla N. Jordan is a Ph.D.~candidate at the University of Texas at
    Austin. Erin M. Buchanan is an Associate Professor of Quantitative
    Psychology at Missouri State University. William E. Padfield is a
    masters degree candidate at Missouri State University.
    
    Correspondence concerning this article should be addressed to Kayla N.
    Jordan, Postal address. E-mail:
    \href{mailto:my@email.com}{\nolinkurl{my@email.com}}
  }


  \abstract{Enter abstract here. Each new line herein must be indented, like this
line.}
  \keywords{keywords \\

    
  }





\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}

\maketitle

\setcounter{secnumdepth}{0}



Examining the construct and measurement validity of psychometric scales
can be difficult, especially for complex constructs such as morality.
Given the pervasiveness of language as avenue of moral justification and
moral argument, it is important to understand how language is indicative
of moral reasoning. Hence, the current study sought to examine the
validity of one approach to measuring moral language using the framework
of moral foundations theory, in comparison to traditional questionnaire
style measurements.

\subsection{Moral Foundations Theory}\label{moral-foundations-theory}

Moral Foundations Theory (MFT) was proposed by Haidt and Joseph (2004)
to explain the differences between political liberals' and
conservatives' moral thinking processes. The differences in party
processing were explained by variable focus on five moral foundations.
The first two of these foundations represents concerns for individuals.
The \emph{harm/care} foundation encompasses concerns of promoting
compassion and/or denigrating cruelty. The \emph{fairness/reciprocity}
foundation covers concerns of ensuring equality and justice. The next
three foundations represent concerns for the group. The
\emph{ingroup/loyalty} foundation encompasses concerns encouraging
patriotism and discouraging dissent. The \emph{authority/respect}
foundation represents concerns maintaining tradition and respecting
social hierarchies. The \emph{purity/sanctity} foundation encompasses
concerns engaging in virtues such as chastity and self-control and
abstaining from vices such as lust and gluttony. Throughout this
manuscript, we will use \emph{harm}, \emph{fairness}, \emph{ingroup},
\emph{authority}, and \emph{purity} to indicate the foundations and
their direction. For example, higher endorsement of the \emph{authority}
foundation implies a focus on basing moral judgments on respecting
tradition and hierarchy, while lower levels of endorsement imply basing
moral judgments less on respecting tradition and hierarchy and more on
other concerns.

The endorsement along these moral foundation continuums is related to
political orientation. Namely, those of liberal political orientation
base moral judgments on the \emph{harm} and \emph{fairness} foundations
whereas those of conservative orientation based judgments on all five
foundations (Federico, Weber, Ergun, \& Hunt, 2013; Graham, Haidt, \&
Nosek, 2009; Graham, Nosek, \& Haidt, 2012; Weber \& Federico, 2013).
Furthermore, Graham et al. (2012) found the differences between the two
sides of the political spectrum were exaggerated by the opposing party.
For example, liberals rated conservatives as more conservative than
conservatives rated themselves and vice versa. In addition to political
orientation, moral foundations also predicted specific policy
preferences and attitudes. Kertzer, Powers, Rathbun, and Iyer (2014)
found that higher endorsement of the \emph{ingroup}, \emph{authority},
and \emph{purity} foundations predicted support for the Iraq War and a
preemptive strike against Iran. However, higher endorsement of
\emph{harm} and \emph{fairness} foundations predicted support for the
Kyoto protocols. Koleva, Graham, Iyer, Ditto, and Haidt (2012) examined
the relationship between moral foundation endorsement and a wide range
of policy attitudes. Greater endorsement of the \emph{harm} foundation
predicted opposition to animal testing, the death penalty, and torture,
as well as support for gun control. Endorsement of the \emph{ingroup}
foundation predicted greater disapproval of flag burning and terrorism,
as well as greater support for defense spending. Finally, stronger
endorsement of \emph{purity} predicted opposition to abortion, same sex
marriage, teaching of evolution, and illegal immigration.

\subsection{Moral Foundations
Questionnaire}\label{moral-foundations-questionnaire}

The Moral Foundations Questionnaire (MFQ) was developed in order to
measure the extent to which an individual endorses each moral foundation
({\textbf{???}}). The MFQ is a 30-item scale divided into two subscales:
moral relevance and moral judgments. The 15 moral relevance items are
equally divided among the five foundations and examine how relevant a
condition is to making a moral judgment on a scale of 1 (\emph{not at
all relevant}) to 6 (\emph{extremely relevant}). These relevance items
include examples such as: \enquote{Whether or not someone used violence
(\emph{harm}),} \enquote{Whether or not someone was denied his or her
rights (\emph{fairness}),} \enquote{Whether or not someone showed a lack
of loyalty (\emph{ingroup}),} \enquote{Whether or not an action caused
chaos or disorder (\emph{authority}),} and \enquote{Whether or not
someone did something disgusting (\emph{purity}).} The moral judgments
items are also equally divided between the foundations and ask on a
six-point scale how much one agrees with each of the statements. These
judgment items include: \enquote{One of the worst things a person can do
is hurt a defenseless animal (\emph{harm}),} \enquote{Justice is the
most important requirement of a society (\emph{fairness}),} \enquote{I
am proud of my country's history (\emph{ingroup}),} \enquote{Men and
women each have different roles to play in society (\emph{authority}),}
and \enquote{Chastity is an important and valuable virtue
(\emph{purity}).}

The internal consistency of this version from ({\textbf{???}}) was
\(\alpha\) = .73 averaged across subscales with a range of \(\alpha\) =
.65-.84. Across six studies, the MFQ was found to have an average
Cronbach's alpha of .63 for \emph{harm}, .64 for \emph{fairness}, .56
for \emph{ingroup}, .59 for \emph{authority}, and .71 for \emph{purity}
(Federico et al., 2013; Graham et al., 2009, 2012; Weber \& Federico,
2013). Test-retest reliability was \emph{r} = .68-.82 using a sample of
123 college students. Confirmatory factor analysis supported a
well-fitted five-factor model (\emph{harm/care},
\emph{fairness/reciprocity}, \emph{ingroup/loyalty},
\emph{authority/respect}, and \emph{purity/sanctity}) over two,
individual (\emph{harm} and \emph{fairness}) versus group
(\emph{ingroup}, \emph{authority}, and \emph{purity}) foundations, or
three, autonomy (\emph{harm}, \emph{fairness}), community
(\emph{ingroup}, \emph{authority}), and divinity (\emph{purity}) ethics,
foundations factor model. The five-factor structure also fit for
non-Western samples, thus, providing evidence of the MFQ
generalizability. Convergent validity was supported with correlations on
other measures of morality ({\textbf{???}}, {\textbf{???}}).

\subsection{Moral Foundations
Dictionary}\label{moral-foundations-dictionary}

Given the importance of language to political ideology and moral
thinking, Graham et al. (2009) developed a moral foundations dictionary
(MFD) to examine the use of moral justification in speech and/or
writing. A dictionary of roughly 50-60 words was developed for each
foundation. Words such as \emph{war} and \emph{peace} should indicate a
greater concern with \emph{harm} foundation whereas words such as
\emph{homeland} and \emph{terrorism} should indicate a greater concern
with the \emph{ingroup} foundation. The other foundation dictionaries
include \emph{equal} and \emph{justice} (\emph{fairness}), \emph{honor}
and \emph{protest} (\emph{authority}), and \emph{holy} and \emph{sin}
(\emph{purity}). To validate the word sets, Graham et al. (2009)
examined the frequency of MFD words in liberal and conservative sermons.
They found liberal ministers used \emph{harm}, \emph{fairness}, and
\emph{ingroup} words more often than conservative ministers who used
\emph{authority} and \emph{purity} words more often. Although
conservative ministers were expected to use more \emph{ingroup} words
based on political ideology and previous research, an examination of the
way liberal ministers used \emph{ingroup} words revealed a tendency for
the use of \emph{ingroup} words to glorify rebellion and promote
independence (i.e., the opposite direction from \emph{ingroup}
definitions). Effect sizes indicated relatively sizable difference
between liberal and conservative sermons with Cohen's \emph{d} ranging
from 0.56 to 1.27.

Graham et al. (2009)'s validation focused on the frequency of moral
words as a dependent variable for the MFD. In contrast to this approach,
({\textbf{???}}) explored how moral words were used paired with other
co-occurring concepts using Latent Semantic Analysis (LSA). They
examined three different moral issues in different contexts to piece out
specific moral words and their collocates. First, they looked at how
moral words were used in relation to the World Trade Center compared to
the Empire State Building in the New York Times from 1987-2007. After
9-11, the number of moral words associated with the World Trade Center
increased, specifically \emph{harm} words from the MFD. Second, they
considered the changes in how moral words were paired with mosque used
in blogs as a response to the debate of building a mosque near Ground
Zero following 9-11. They found words from the MFD were used more often
with mosque during the main debate and then the co-occurrence decreased
afterwards. Lastly, they examined moral language tied to the abortion
debate in Congress. Republicans used more moral language overall; more
specifically, Republicans tended to use more words associated with the
\emph{purity} foundation; while Democrats used more words associated
with the \emph{fairness} foundation.\\
These studies are the first steps at supporting the moral foundations
dictionary and questionnaire using the moral foundations framework. This
study combined both the dictionary and questionnaire to expand the
literature on their usefulness and psychometric properties due to the
dearth of studies on both measures. Therefore, the purpose of the
current study was to explore the reliability and validity of the MFD and
MFQ using the following procedures:

****1) Cronbach's \(\alpha\) of both measurement tools, as previous
studies have shown mixed reliabilities 2) a multi-method, multi-trait
(MTMM) design comparing the MFD and MFQ on one sample, and 3) the
predictive validity of the MFD and MFQ to political orientation using
Congressional speech records. ****

going to end up editing here after we finish the four pronged approach
part.

\section{Experiment 1}\label{experiment-1}

\section{Method}\label{method}

\subsection{Particpants}\label{particpants}

466 participants were collected from a large Midwestern university.
Participants were given course credit for their introductory psychology
course for completing the study. 451 participants had less than five
percent missing data and were retained for analyses. Participants were
asked to denote their political party, and 26.4\% indicated they were
Democrats, 42.6\% were Republican, and 31.0\% indicated they were
Independent.

\section{Materials and Procedure}\label{materials-and-procedure}

A complete example of the survey can be found online at OSF LINK. First,
participants were given a description of associative memory as the
relation between words that comes about through many pairings in writing
and speech. Next, the free association task, similar to that used in
({\textbf{???}}) and ({\textbf{???}}) was described to the participants
as listing the \enquote{first word that pops into mind}. The
participants were then given three example free association cues,
\emph{lost}, \emph{old}, and \emph{article}. For each cue, participants
were asked to write all the words that come to mind. To elicit free
association to the moral foundation areas, participants were given the
following instructions:

\enquote{Moral Foundations Theory states that when making moral
judgments/decisions, the concerns people have can be divided into five
categories. Below are labels of each of these five categories. You will
then be asked to list words you think are associated with each of the
labels.}

Each of the foundation pairs were listed together (i.e.
\emph{harm/care}, \emph{fairness/cheating}) with a space for
participants to list their free association concepts. After the free
association task, participants were then given the 15-items from the
moral relevance section of the MFQ as described in the introduction.
Last, participants were asked to denote their political orientation from
1 \emph{conservative} to 10 \emph{liberal}, as well as which political
party they associated with: Democrat, Republican, and Independent. The
survey was delivered through Qualtrics, and participants were recruited
through the online participant management system for the university
(SONA). Each participant signed an online consent form at the beginning
of the study and was given participation credit at the end of the study.

\section{Results}\label{results}

All data was screening for inaccurate responses, as well as missing
data, as described in the participant section. Two participants were
missing data on the political orientation scale after excluding
participants with more than five percent missing data, and this data was
excluded pairwise. The MFQ data was screened for multivariate outliers
with Mahalanobis distance as described in ({\textbf{???}}), and fourteen
outliers were found using \(\chi^2_{p < .001}\)(15) = 37.70. These
participants were excluded from further analyses, representing 437 final
participants. The final data was screened for assumptions of normality,
linearity, homogeneity and homoscedasticity, and these were found to be
satisfactory.

The sum of each moral foundation area was calculated in order to
determine which words were linked to their respective moral foundation.
The average scores were: harm (\emph{M} = 14.16, \emph{SD} = 2.44),
fairness (\emph{M} = 14.31, \emph{SD} = 2.60), ingroup (\emph{M} =
12.74, \emph{SD} = 2.91), authority (\emph{M} = 12.06, \emph{SD} =
3.01), and purity (\emph{M} = 11.58, \emph{SD} = 3.36). Participants
free association responses were processed using the \emph{tm} library
({\textbf{???}}) after spell checking. Each set of answers was cleaned
for punctuation, English stop words (e.g., \emph{the}, \emph{an},
\emph{of}) were removed, and each word was stemmed using the English
library in \emph{tm}. We did not combine related words in this section
(i.e., \emph{injure} and \emph{injury}, which have different stems
\emph{injur} and \emph{injuri}) to allow for maximum coverage of
different word forms present in the dictionary. Additionally, with the
use of automated stemmers like that present in the \emph{tm} library,
leaving both word forms in the dictionary would capture more of the
concepts present in future analyses with a different corpus without the
requirement on the experimenter to manuall recode all word forms.
Frequency counts of the stemmed words were tabulated and only words
mentioned with at least one percent frequency were used in the
subsequent analyses. The complete set of word frequencies for each
foundation can be found in our supplemental materials.

This procedure generated a large frequency of words for a new dictionary
of moral foundations: harm 96, fairness 76, ingroup 86, authority 81,
and purity 80. These concepts were included in the full dictionary used
for Experiment 3. We additionally created a reduced dictionary that
included only concepts correlated with their respective moral
foundations scores. Correlations between word frequency and the sum of
the MFQ were calculated for each foundation and set of concepts. Words
were included in the reduced dictionary if their correlation was two
standard deviations away from the mean correlation for that foundation.
The reduced dataset included the following number of words for each
foundation: harm 4, fairness 3, ingroup 3, authority 4, and purity 2.

\section{Experiment 2}\label{experiment-2}

\section{Method}\label{method-1}

\subsection{Participants}\label{participants}

Participants were recruited in two waves as part of a larger
investigation on priming political and religious attitudes
({\textbf{???}}). Participants were recruited via an online research
system (SONA) and were given course credit for their participation. 463
participants were included in the this study. The study was mostly women
(53.9\%) and White (76.4\%) participants with a mix of minority
participants: Black (6.1\%), (3.6\%), (4.2\%), Native American (1.9\%),
Mixed (2.9\%) and Other (4.9\%). The average listed age was 19.75
(\emph{SD} = 2.94).

\subsection{Materials and Procedure}\label{materials-and-procedure-1}

Data was againt collected via Qualtrics. Four fake new stories were
presented to participants, which were roughly 400 words each. First, all
news stories included a few sentences describing the use of chemical
weapons in the Syrian civil war. The news stories were manipulated with
political (Republican v. Democrat) and religious (religious v. not)
quotes in a 2 x 2 design. News stories can be found in the online
materials. Participants also completed the 30-item version of the MFQ as
described in the introduction. In addition to basic demographics
(gender, age), participant political orientation was assessed with the
same scale described in Experiment 1.

After consenting to participate in the study, participants were randomly
shown one of the four new articles about Syria's use of chemical
weapons. Participants were then asked to write for 5-10 minutes about
their reaction to Syria's use of chemical weapons and the needed
response from the United States. The second wave of data collection
included different writing prompts designed to capture more of the moral
foundation areas in their writing. The following writing prompt was
used, \enquote{Please write about your attitudes on abortion (or
same-sex marriage or environmentalism) as well as your reason for this
stance.} The three prompts were chosen to create a more varied word set
by using topics that should elicit words from each moral foundations
category by soliciting a moral response. In each wave of data collection
(Syria prompts, moral prompts) the prompt material was randomized
between participants. Participants then completed the MFQ, demographics,
and the political orientation scale.

\section{Results}\label{results-1}

Participant data was first spell checked and screening for inaccurate
responses. Participants who did not write more than fifty words in
response to a given prompt were excluded (\emph{n} = 81). One missing
datapoint was estimated using the \emph{mice} library from \emph{R}
({\textbf{???}}) for a missing MFQ question, and all other missing data
was present in the demographics sections, which were not filled in. The
MFQ data were screened for outliers using Mahalanobis distance, and
fourteen outliers were found at \(\chi^2_{p<.001}\)(30) = 59.70. These
data were excluded leading to a final sample size of 368. Data were
screened for assumptions described in Experiment 1 and were found to be
satisfactory.

In the first study, only free association responses were collected, but
in this study, full writing samples were collected. Therefore, we
expected many of the words listed to be part of creating a cohesive
discourse, rather than only related to the moral foundation targeted. To
find only the most related words, the correlation between word frequency
and moral foundation was calculated and words with correlations greater
than two standard deviations outside the mean were selected for the
dictionary analysis in Experiment 3. The sum of each moral foundation
area was calculated in order to determine which words were linked to
their respective moral foundation. The average scores were: harm
(\emph{M} = 13.95, \emph{SD} = 2.73), fairness (\emph{M} = 14.26,
\emph{SD} = 2.71), ingroup (\emph{M} = 11.97, \emph{SD} = 3.25),
authority (\emph{M} = 11.57, \emph{SD} = 3.00), and purity (\emph{M} =
11.68, \emph{SD} = 3.63).

\subsection{DEFINITELY STOPPED HERE}\label{definitely-stopped-here}

\section{Experiment 3}\label{experiment-3}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{##find very small words}
\NormalTok{columndata =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata, }\DecValTok{2}\NormalTok{, sum) }
\NormalTok{correldata2 =}\StringTok{ }\NormalTok{correldata[ , columndata }\OperatorTok{>}\StringTok{ }\DecValTok{5}\NormalTok{]}

\NormalTok{allcorrels =}\StringTok{ }\KeywordTok{cor}\NormalTok{(correldata2[ , }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\OperatorTok{:}\KeywordTok{ncol}\NormalTok{(correldata2))])}
\NormalTok{imptcorrels =}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(allcorrels[}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{,(}\KeywordTok{nrow}\NormalTok{(allcorrels)}\OperatorTok{-}\DecValTok{29}\NormalTok{)}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(allcorrels)) , }\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{])}

\NormalTok{M =}\StringTok{ }\KeywordTok{apply}\NormalTok{(imptcorrels, }\DecValTok{2}\NormalTok{, mean)}
\NormalTok{SD =}\StringTok{ }\KeywordTok{apply}\NormalTok{(imptcorrels, }\DecValTok{2}\NormalTok{, sd)}

\NormalTok{cutoffH =}\StringTok{ }\NormalTok{M }\OperatorTok{+}\StringTok{ }\DecValTok{2}\OperatorTok{*}\NormalTok{SD}
\NormalTok{cutoffL =}\StringTok{ }\NormalTok{M }\OperatorTok{-}\StringTok{ }\DecValTok{2}\OperatorTok{*}\NormalTok{SD}

\NormalTok{harmwords =}\StringTok{ }\KeywordTok{rownames}\NormalTok{(}\KeywordTok{subset}\NormalTok{(imptcorrels, harm }\OperatorTok{>}\StringTok{ }\NormalTok{cutoffH[}\StringTok{"harm"}\NormalTok{] }\OperatorTok{|}\StringTok{ }\NormalTok{harm }\OperatorTok{<}\StringTok{ }\NormalTok{cutoffL[}\StringTok{"harm"}\NormalTok{]))}
\NormalTok{fairwords =}\StringTok{ }\KeywordTok{rownames}\NormalTok{(}\KeywordTok{subset}\NormalTok{(imptcorrels, fair }\OperatorTok{>}\StringTok{ }\NormalTok{cutoffH[}\StringTok{"fair"}\NormalTok{] }\OperatorTok{|}\StringTok{ }\NormalTok{fair }\OperatorTok{<}\StringTok{ }\NormalTok{cutoffL[}\StringTok{"fair"}\NormalTok{]))}
\NormalTok{ingroupwords =}\StringTok{ }\KeywordTok{rownames}\NormalTok{(}\KeywordTok{subset}\NormalTok{(imptcorrels, ingroup }\OperatorTok{>}\StringTok{ }\NormalTok{cutoffH[}\StringTok{"ingroup"}\NormalTok{] }\OperatorTok{|}\StringTok{ }\NormalTok{ingroup }\OperatorTok{<}\StringTok{ }\NormalTok{cutoffL[}\StringTok{"ingroup"}\NormalTok{]))}
\NormalTok{authoritywords =}\StringTok{ }\KeywordTok{rownames}\NormalTok{(}\KeywordTok{subset}\NormalTok{(imptcorrels, authority }\OperatorTok{>}\StringTok{ }\NormalTok{cutoffH[}\StringTok{"authority"}\NormalTok{] }\OperatorTok{|}\StringTok{ }\NormalTok{authority }\OperatorTok{<}\StringTok{ }\NormalTok{cutoffL[}\StringTok{"authority"}\NormalTok{]))}
\NormalTok{puritywords =}\StringTok{ }\KeywordTok{rownames}\NormalTok{(}\KeywordTok{subset}\NormalTok{(imptcorrels, purity }\OperatorTok{>}\StringTok{ }\NormalTok{cutoffH[}\StringTok{"purity"}\NormalTok{] }\OperatorTok{|}\StringTok{ }\NormalTok{purity }\OperatorTok{<}\StringTok{ }\NormalTok{cutoffL[}\StringTok{"purity"}\NormalTok{]))}

\NormalTok{##the amount of times people used the words in THIS dataset on Syria}
\NormalTok{mtmmdata =}\StringTok{ }\NormalTok{correldata[ , }\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{]}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{h1 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , harmwords], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{f1 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , fairwords], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{i1 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , ingroupwords], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{a1 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , authoritywords], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{p1 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , puritywords], }\DecValTok{1}\NormalTok{, sum)}

\NormalTok{##the amount of time people used the original MFD words}
\NormalTok{original_mfd =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"original_mfd.csv"}\NormalTok{, }\DataTypeTok{stringsAsFactors =}\NormalTok{ F)}

\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{h2 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , original_mfd}\OperatorTok{$}\NormalTok{h2[}\DecValTok{1}\OperatorTok{:}\DecValTok{26}\NormalTok{] ], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{f2 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , original_mfd}\OperatorTok{$}\NormalTok{f2[}\DecValTok{1}\OperatorTok{:}\DecValTok{19}\NormalTok{] ], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{i2 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , original_mfd}\OperatorTok{$}\NormalTok{i2[}\DecValTok{1}\OperatorTok{:}\DecValTok{15}\NormalTok{] ], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{a2 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , original_mfd}\OperatorTok{$}\NormalTok{a2[}\DecValTok{1}\OperatorTok{:}\DecValTok{30}\NormalTok{] ], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{p2 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , original_mfd}\OperatorTok{$}\NormalTok{p2[}\DecValTok{1}\OperatorTok{:}\DecValTok{20}\NormalTok{] ], }\DecValTok{1}\NormalTok{, sum)}

\NormalTok{##the amount of times people used the new dictionary words from the norming study}
\NormalTok{new_data =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"new_data.csv"}\NormalTok{, }\DataTypeTok{stringsAsFactors =}\NormalTok{ F)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{h3 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , new_data}\OperatorTok{$}\NormalTok{h3[}\DecValTok{1}\OperatorTok{:}\DecValTok{24}\NormalTok{] ], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{f3 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , new_data}\OperatorTok{$}\NormalTok{f3[}\DecValTok{1}\OperatorTok{:}\DecValTok{21}\NormalTok{] ], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{i3 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , new_data}\OperatorTok{$}\NormalTok{i3[}\DecValTok{1}\OperatorTok{:}\DecValTok{11}\NormalTok{] ], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{a3 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , new_data}\OperatorTok{$}\NormalTok{a3[}\DecValTok{1}\OperatorTok{:}\DecValTok{16}\NormalTok{] ], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{p3 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , new_data}\OperatorTok{$}\NormalTok{p3[}\DecValTok{1}\OperatorTok{:}\DecValTok{21}\NormalTok{] ], }\DecValTok{1}\NormalTok{, sum)}

\NormalTok{##remember that original data is number 2}

\NormalTok{##intersection data original + 1}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{h12 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , }\KeywordTok{unique}\NormalTok{(}\KeywordTok{c}\NormalTok{(original_mfd}\OperatorTok{$}\NormalTok{h2[}\DecValTok{1}\OperatorTok{:}\DecValTok{26}\NormalTok{], harmwords))], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{f12 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , }\KeywordTok{unique}\NormalTok{(}\KeywordTok{c}\NormalTok{(original_mfd}\OperatorTok{$}\NormalTok{f2[}\DecValTok{1}\OperatorTok{:}\DecValTok{19}\NormalTok{], fairwords)) ], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{i12 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , }\KeywordTok{unique}\NormalTok{(}\KeywordTok{c}\NormalTok{(original_mfd}\OperatorTok{$}\NormalTok{i2[}\DecValTok{1}\OperatorTok{:}\DecValTok{15}\NormalTok{], ingroupwords)) ], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{a12 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , }\KeywordTok{unique}\NormalTok{(}\KeywordTok{c}\NormalTok{(original_mfd}\OperatorTok{$}\NormalTok{a2[}\DecValTok{1}\OperatorTok{:}\DecValTok{30}\NormalTok{], authoritywords)) ], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{p12 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , }\KeywordTok{unique}\NormalTok{(}\KeywordTok{c}\NormalTok{(original_mfd}\OperatorTok{$}\NormalTok{p2[}\DecValTok{1}\OperatorTok{:}\DecValTok{20}\NormalTok{], puritywords)) ], }\DecValTok{1}\NormalTok{, sum)}

\NormalTok{##intersection data original + 3}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{h23 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , }\KeywordTok{unique}\NormalTok{(}\KeywordTok{c}\NormalTok{(original_mfd}\OperatorTok{$}\NormalTok{h2[}\DecValTok{1}\OperatorTok{:}\DecValTok{26}\NormalTok{], new_data}\OperatorTok{$}\NormalTok{h3[}\DecValTok{1}\OperatorTok{:}\DecValTok{24}\NormalTok{]))], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{f23 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , }\KeywordTok{unique}\NormalTok{(}\KeywordTok{c}\NormalTok{(original_mfd}\OperatorTok{$}\NormalTok{f2[}\DecValTok{1}\OperatorTok{:}\DecValTok{19}\NormalTok{], new_data}\OperatorTok{$}\NormalTok{f3[}\DecValTok{1}\OperatorTok{:}\DecValTok{21}\NormalTok{])) ], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{i23 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , }\KeywordTok{unique}\NormalTok{(}\KeywordTok{c}\NormalTok{(original_mfd}\OperatorTok{$}\NormalTok{i2[}\DecValTok{1}\OperatorTok{:}\DecValTok{15}\NormalTok{], new_data}\OperatorTok{$}\NormalTok{i3[}\DecValTok{1}\OperatorTok{:}\DecValTok{11}\NormalTok{])) ], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{a23 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , }\KeywordTok{unique}\NormalTok{(}\KeywordTok{c}\NormalTok{(original_mfd}\OperatorTok{$}\NormalTok{a2[}\DecValTok{1}\OperatorTok{:}\DecValTok{30}\NormalTok{], new_data}\OperatorTok{$}\NormalTok{a3[}\DecValTok{1}\OperatorTok{:}\DecValTok{16}\NormalTok{] )) ], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{p23 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , }\KeywordTok{unique}\NormalTok{(}\KeywordTok{c}\NormalTok{(original_mfd}\OperatorTok{$}\NormalTok{p2[}\DecValTok{1}\OperatorTok{:}\DecValTok{20}\NormalTok{], new_data}\OperatorTok{$}\NormalTok{p3[}\DecValTok{1}\OperatorTok{:}\DecValTok{21}\NormalTok{] )) ], }\DecValTok{1}\NormalTok{, sum)}

\NormalTok{##intersection data all}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{h123 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , }\KeywordTok{unique}\NormalTok{(}\KeywordTok{c}\NormalTok{(original_mfd}\OperatorTok{$}\NormalTok{h2[}\DecValTok{1}\OperatorTok{:}\DecValTok{26}\NormalTok{], harmwords, new_data}\OperatorTok{$}\NormalTok{h3[}\DecValTok{1}\OperatorTok{:}\DecValTok{24}\NormalTok{]))], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{f123 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , }\KeywordTok{unique}\NormalTok{(}\KeywordTok{c}\NormalTok{(original_mfd}\OperatorTok{$}\NormalTok{f2[}\DecValTok{1}\OperatorTok{:}\DecValTok{19}\NormalTok{], fairwords, new_data}\OperatorTok{$}\NormalTok{f3[}\DecValTok{1}\OperatorTok{:}\DecValTok{21}\NormalTok{])) ], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{i123 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , }\KeywordTok{unique}\NormalTok{(}\KeywordTok{c}\NormalTok{(original_mfd}\OperatorTok{$}\NormalTok{i2[}\DecValTok{1}\OperatorTok{:}\DecValTok{15}\NormalTok{], ingroupwords, new_data}\OperatorTok{$}\NormalTok{i3[}\DecValTok{1}\OperatorTok{:}\DecValTok{11}\NormalTok{])) ], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{a123 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , }\KeywordTok{unique}\NormalTok{(}\KeywordTok{c}\NormalTok{(original_mfd}\OperatorTok{$}\NormalTok{a2[}\DecValTok{1}\OperatorTok{:}\DecValTok{30}\NormalTok{], authoritywords, new_data}\OperatorTok{$}\NormalTok{a3[}\DecValTok{1}\OperatorTok{:}\DecValTok{16}\NormalTok{] )) ], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata}\OperatorTok{$}\NormalTok{p123 =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , }\KeywordTok{unique}\NormalTok{(}\KeywordTok{c}\NormalTok{(original_mfd}\OperatorTok{$}\NormalTok{p2[}\DecValTok{1}\OperatorTok{:}\DecValTok{20}\NormalTok{], puritywords, new_data}\OperatorTok{$}\NormalTok{p3[}\DecValTok{1}\OperatorTok{:}\DecValTok{21}\NormalTok{] )) ], }\DecValTok{1}\NormalTok{, sum)}

\NormalTok{##normalize the whole damn thing}
\NormalTok{totalwords =}\StringTok{ }\KeywordTok{apply}\NormalTok{(correldata[ , }\DecValTok{7}\OperatorTok{:}\KeywordTok{ncol}\NormalTok{(correldata)], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{mtmmdata[ , }\DecValTok{7}\OperatorTok{:}\KeywordTok{ncol}\NormalTok{(mtmmdata)] =}\StringTok{ }\NormalTok{mtmmdata[ , }\DecValTok{7}\OperatorTok{:}\KeywordTok{ncol}\NormalTok{(mtmmdata)]}\OperatorTok{/}\NormalTok{totalwords}\OperatorTok{*}\DecValTok{100}
\NormalTok{mtmmdata =}\StringTok{ }\KeywordTok{cbind}\NormalTok{(mtmmdata, correldata[ , }\DecValTok{2049}\OperatorTok{:}\KeywordTok{ncol}\NormalTok{(correldata)])}

\NormalTok{##now run some MTMM!}
\KeywordTok{library}\NormalTok{(semPlot)}
\KeywordTok{library}\NormalTok{(lavaan)}

\NormalTok{####mtmm our data correlation 1####}
\NormalTok{model1 =}\StringTok{ '}
\StringTok{harmL =~ X1+X2+X3+X4+X5+X6+h1}
\StringTok{fairL =~ X7+X8+X9+X10+X11+X12+f1}
\StringTok{ingroupL =~ X13+X14+X15+X16+X17+X18+i1}
\StringTok{authorityL =~ X19+X20+X21+X22+X23+X24+a1}
\StringTok{purityL=~ X25+X26+X27+X28+X29+X30+p1}
\StringTok{mfq =~ X1+X2+X3+X4+X5+X6+X7+X8+X9+X10+X11+X12+X13+X14+X15+X16+X17+X18+X19+X20+X21+X22+X23+X24+X25+X26+X27+X28+X29+X30}
\StringTok{mfd =~ h1+f1+i1+a1+p1}

\StringTok{##fix the covariances}
\StringTok{harmL~~0*mfq}
\StringTok{fairL~~0*mfq}
\StringTok{ingroupL~~0*mfq}
\StringTok{authorityL~~0*mfq}
\StringTok{purityL~~0*mfq}
\StringTok{harmL~~0*mfd}
\StringTok{fairL~~0*mfd}
\StringTok{ingroupL~~0*mfd}
\StringTok{authorityL~~0*mfd}
\StringTok{purityL~~0*mfd}
\StringTok{ingroupL~~-.29*harmL}
\StringTok{f1~~1.35*f1}
\StringTok{'}

\NormalTok{model1.fit =}\StringTok{ }\KeywordTok{cfa}\NormalTok{(model1, }\DataTypeTok{data=}\NormalTok{mtmmdata, }\DataTypeTok{std.lv=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(model1.fit, }\DataTypeTok{rsquare=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{standardized=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{semPaths}\NormalTok{(model1.fit, }\DataTypeTok{whatLabels =} \StringTok{"std"}\NormalTok{, }\DataTypeTok{layout =} \StringTok{"tree"}\NormalTok{)}
\KeywordTok{fitMeasures}\NormalTok{(model1.fit, }\DataTypeTok{fit.measures =} \StringTok{"aic"}\NormalTok{)}


\NormalTok{####mtmm original data 2####}
\NormalTok{model2 =}\StringTok{ '}
\StringTok{harmL =~ X1+X2+X3+X4+X5+X6+h2}
\StringTok{fairL =~ X7+X8+X9+X10+X11+X12+f2}
\StringTok{ingroupL =~ X13+X14+X15+X16+X17+X18+i2}
\StringTok{authorityL =~ X19+X20+X21+X22+X23+X24+a2}
\StringTok{purityL=~ X25+X26+X27+X28+X29+X30+p2}
\StringTok{mfq =~ X1+X2+X3+X4+X5+X6+X7+X8+X9+X10+X11+X12+X13+X14+X15+X16+X17+X18+X19+X20+X21+X22+X23+X24+X25+X26+X27+X28+X29+X30}
\StringTok{mfd =~ h2+f2+i2+a2+p2}

\StringTok{##fix the covariances}
\StringTok{harmL~~0*mfq}
\StringTok{fairL~~0*mfq}
\StringTok{ingroupL~~0*mfq}
\StringTok{authorityL~~0*mfq}
\StringTok{purityL~~0*mfq}
\StringTok{harmL~~0*mfd}
\StringTok{fairL~~0*mfd}
\StringTok{ingroupL~~0*mfd}
\StringTok{authorityL~~0*mfd}
\StringTok{purityL~~0*mfd}
\StringTok{a2~~1.56*a2}
\StringTok{h2~~2.14*h2}
\StringTok{f2~~1.84*f2}
\StringTok{'}

\NormalTok{model2.fit =}\StringTok{ }\KeywordTok{cfa}\NormalTok{(model2, }\DataTypeTok{data=}\NormalTok{mtmmdata, }\DataTypeTok{std.lv=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(model2.fit, }\DataTypeTok{rsquare=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{standardized=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{fitMeasures}\NormalTok{(model2.fit, }\DataTypeTok{fit.measures =} \StringTok{"aic"}\NormalTok{)}

\NormalTok{####mtmm participant word data 3####}
\NormalTok{model3 =}\StringTok{ '}
\StringTok{harmL =~ X1+X2+X3+X4+X5+X6+h3}
\StringTok{fairL =~ X7+X8+X9+X10+X11+X12+f3}
\StringTok{ingroupL =~ X13+X14+X15+X16+X17+X18+i3}
\StringTok{authorityL =~ X19+X20+X21+X22+X23+X24+a3}
\StringTok{purityL=~ X25+X26+X27+X28+X29+X30+p3}
\StringTok{mfq =~ X1+X2+X3+X4+X5+X6+X7+X8+X9+X10+X11+X12+X13+X14+X15+X16+X17+X18+X19+X20+X21+X22+X23+X24+X25+X26+X27+X28+X29+X30}
\StringTok{mfd =~ h3+f3+i3+a3+p3}

\StringTok{##fix the covariances}
\StringTok{harmL~~0*mfq}
\StringTok{fairL~~0*mfq}
\StringTok{ingroupL~~0*mfq}
\StringTok{authorityL~~0*mfq}
\StringTok{purityL~~0*mfq}
\StringTok{harmL~~0*mfd}
\StringTok{fairL~~0*mfd}
\StringTok{ingroupL~~0*mfd}
\StringTok{authorityL~~0*mfd}
\StringTok{purityL~~0*mfd}
\StringTok{h3~~.66*h3}
\StringTok{i3~~.56*i3}
\StringTok{f3~~1.97*f3}
\StringTok{'}

\NormalTok{model3.fit =}\StringTok{ }\KeywordTok{cfa}\NormalTok{(model3, }\DataTypeTok{data=}\NormalTok{mtmmdata, }\DataTypeTok{std.lv=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(model3.fit, }\DataTypeTok{rsquare=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{standardized=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{fitMeasures}\NormalTok{(model3.fit, }\DataTypeTok{fit.measures =} \StringTok{"aic"}\NormalTok{)}

\NormalTok{####mtmm model 1 and 2 together####}
\NormalTok{model4 =}\StringTok{ '}
\StringTok{harmL =~ X1+X2+X3+X4+X5+X6+h12}
\StringTok{fairL =~ X7+X8+X9+X10+X11+X12+f12}
\StringTok{ingroupL =~ X13+X14+X15+X16+X17+X18+i12}
\StringTok{authorityL =~ X19+X20+X21+X22+X23+X24+a12}
\StringTok{purityL=~ X25+X26+X27+X28+X29+X30+p12}
\StringTok{mfq =~ X1+X2+X3+X4+X5+X6+X7+X8+X9+X10+X11+X12+X13+X14+X15+X16+X17+X18+X19+X20+X21+X22+X23+X24+X25+X26+X27+X28+X29+X30}
\StringTok{mfd =~ h12+f12+i12+a12+p12}

\StringTok{##fix the covariances}
\StringTok{harmL~~0*mfq}
\StringTok{fairL~~0*mfq}
\StringTok{ingroupL~~0*mfq}
\StringTok{authorityL~~0*mfq}
\StringTok{purityL~~0*mfq}
\StringTok{harmL~~0*mfd}
\StringTok{fairL~~0*mfd}
\StringTok{ingroupL~~0*mfd}
\StringTok{authorityL~~0*mfd}
\StringTok{purityL~~0*mfd}
\StringTok{authorityL~~.5*purityL}
\StringTok{'}

\NormalTok{model4.fit =}\StringTok{ }\KeywordTok{cfa}\NormalTok{(model4, }\DataTypeTok{data=}\NormalTok{mtmmdata, }\DataTypeTok{std.lv=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(model4.fit, }\DataTypeTok{rsquare=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{standardized=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{fitMeasures}\NormalTok{(model4.fit, }\DataTypeTok{fit.measures =} \StringTok{"aic"}\NormalTok{)}

\NormalTok{####mtmm model 2 and 3 together####}
\NormalTok{model5 =}\StringTok{ '}
\StringTok{harmL =~ X1+X2+X3+X4+X5+X6+h23}
\StringTok{fairL =~ X7+X8+X9+X10+X11+X12+f23}
\StringTok{ingroupL =~ X13+X14+X15+X16+X17+X18+i23}
\StringTok{authorityL =~ X19+X20+X21+X22+X23+X24+a23}
\StringTok{purityL=~ X25+X26+X27+X28+X29+X30+p23}
\StringTok{mfq =~ X1+X2+X3+X4+X5+X6+X7+X8+X9+X10+X11+X12+X13+X14+X15+X16+X17+X18+X19+X20+X21+X22+X23+X24+X25+X26+X27+X28+X29+X30}
\StringTok{mfd =~ h23+f23+i23+a23+p23}

\StringTok{##fix the covariances}
\StringTok{harmL~~0*mfq}
\StringTok{fairL~~0*mfq}
\StringTok{ingroupL~~0*mfq}
\StringTok{authorityL~~0*mfq}
\StringTok{purityL~~0*mfq}
\StringTok{harmL~~0*mfd}
\StringTok{fairL~~0*mfd}
\StringTok{ingroupL~~0*mfd}
\StringTok{authorityL~~0*mfd}
\StringTok{purityL~~0*mfd}
\StringTok{h23~~2.07*h23}
\StringTok{a23~~1.70*a23}
\StringTok{'}

\NormalTok{model5.fit =}\StringTok{ }\KeywordTok{cfa}\NormalTok{(model5, }\DataTypeTok{data=}\NormalTok{mtmmdata, }\DataTypeTok{std.lv=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(model5.fit, }\DataTypeTok{rsquare=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{standardized=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{fitMeasures}\NormalTok{(model5.fit, }\DataTypeTok{fit.measures =} \StringTok{"aic"}\NormalTok{)}

\NormalTok{####mtmm model 1 2 and 3 together####}
\NormalTok{model6 =}\StringTok{ '}
\StringTok{harmL =~ X1+X2+X3+X4+X5+X6+h123}
\StringTok{fairL =~ X7+X8+X9+X10+X11+X12+f123}
\StringTok{ingroupL =~ X13+X14+X15+X16+X17+X18+i123}
\StringTok{authorityL =~ X19+X20+X21+X22+X23+X24+a123}
\StringTok{purityL=~ X25+X26+X27+X28+X29+X30+p123}
\StringTok{mfq =~ X1+X2+X3+X4+X5+X6+X7+X8+X9+X10+X11+X12+X13+X14+X15+X16+X17+X18+X19+X20+X21+X22+X23+X24+X25+X26+X27+X28+X29+X30}
\StringTok{mfd =~ h123+f123+i123+a123+p123}

\StringTok{##fix the covariances}
\StringTok{harmL~~0*mfq}
\StringTok{fairL~~0*mfq}
\StringTok{ingroupL~~0*mfq}
\StringTok{authorityL~~0*mfq}
\StringTok{purityL~~0*mfq}
\StringTok{harmL~~0*mfd}
\StringTok{fairL~~0*mfd}
\StringTok{ingroupL~~0*mfd}
\StringTok{authorityL~~0*mfd}
\StringTok{purityL~~0*mfd}
\StringTok{authorityL~~.80*purityL}
\StringTok{'}

\NormalTok{model6.fit =}\StringTok{ }\KeywordTok{cfa}\NormalTok{(model6, }\DataTypeTok{data=}\NormalTok{mtmmdata, }\DataTypeTok{std.lv=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(model6.fit, }\DataTypeTok{rsquare=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{standardized=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{fitMeasures}\NormalTok{(model6.fit, }\DataTypeTok{fit.measures =} \StringTok{"aic"}\NormalTok{)}


\NormalTok{####focus on final model####}
\NormalTok{##traits only model}
\NormalTok{model6.}\DecValTok{1}\NormalTok{ =}\StringTok{ '}
\StringTok{harmL =~ X1+X2+X3+X4+X5+X6+h123}
\StringTok{fairL =~ X7+X8+X9+X10+X11+X12+f123}
\StringTok{ingroupL =~ X13+X14+X15+X16+X17+X18+i123}
\StringTok{authorityL =~ X19+X20+X21+X22+X23+X24+a123}
\StringTok{purityL=~ X25+X26+X27+X28+X29+X30+p123}
\StringTok{'}
\NormalTok{model6.}\FloatTok{1.}\NormalTok{fit =}\StringTok{ }\KeywordTok{cfa}\NormalTok{(model6.}\DecValTok{1}\NormalTok{, }\DataTypeTok{data=}\NormalTok{mtmmdata, }\DataTypeTok{std.lv=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(model6.}\FloatTok{1.}\NormalTok{fit, }\DataTypeTok{rsquare=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{standardized=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{fitMeasures}\NormalTok{(model6.}\FloatTok{1.}\NormalTok{fit)}

\NormalTok{##perfectly correlated traits}
\NormalTok{model6.}\DecValTok{2}\NormalTok{ =}\StringTok{ '}
\StringTok{harmL =~ X1+X2+X3+X4+X5+X6+h123}
\StringTok{fairL =~ X7+X8+X9+X10+X11+X12+f123}
\StringTok{ingroupL =~ X13+X14+X15+X16+X17+X18+i123}
\StringTok{authorityL =~ X19+X20+X21+X22+X23+X24+a123}
\StringTok{purityL=~ X25+X26+X27+X28+X29+X30+p123}
\StringTok{mfq =~ X1+X2+X3+X4+X5+X6+X7+X8+X9+X10+X11+X12+X13+X14+X15+X16+X17+X18+X19+X20+X21+X22+X23+X24+X25+X26+X27+X28+X29+X30}
\StringTok{mfd =~ h123+f123+i123+a123+p123}

\StringTok{##fix the covariances}
\StringTok{harmL~~0*mfq}
\StringTok{fairL~~0*mfq}
\StringTok{ingroupL~~0*mfq}
\StringTok{authorityL~~0*mfq}
\StringTok{purityL~~0*mfq}
\StringTok{harmL~~0*mfd}
\StringTok{fairL~~0*mfd}
\StringTok{ingroupL~~0*mfd}
\StringTok{authorityL~~0*mfd}
\StringTok{purityL~~0*mfd}

\StringTok{harmL~~1*fairL}
\StringTok{harmL~~1*authorityL}
\StringTok{harmL~~1*purityL}
\StringTok{harmL~~1*ingroupL}
\StringTok{fairL~~1*authorityL}
\StringTok{fairL~~1*purityL}
\StringTok{fairL~~1*ingroupL}
\StringTok{authorityL~~1*purityL}
\StringTok{authorityL~~1*ingroupL}
\StringTok{purityL~~1*ingroupL}
\StringTok{'}

\NormalTok{model6.}\FloatTok{2.}\NormalTok{fit =}\StringTok{ }\KeywordTok{cfa}\NormalTok{(model6.}\DecValTok{2}\NormalTok{, }\DataTypeTok{data=}\NormalTok{mtmmdata, }\DataTypeTok{std.lv=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(model6.}\FloatTok{2.}\NormalTok{fit, }\DataTypeTok{rsquare=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{standardized=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{fitMeasures}\NormalTok{(model6.}\FloatTok{2.}\NormalTok{fit)}

\NormalTok{##no method correl}
\NormalTok{model6.}\DecValTok{3}\NormalTok{ =}\StringTok{ '}
\StringTok{harmL =~ X1+X2+X3+X4+X5+X6+h123}
\StringTok{fairL =~ X7+X8+X9+X10+X11+X12+f123}
\StringTok{ingroupL =~ X13+X14+X15+X16+X17+X18+i123}
\StringTok{authorityL =~ X19+X20+X21+X22+X23+X24+a123}
\StringTok{purityL=~ X25+X26+X27+X28+X29+X30+p123}
\StringTok{mfq =~ X1+X2+X3+X4+X5+X6+X7+X8+X9+X10+X11+X12+X13+X14+X15+X16+X17+X18+X19+X20+X21+X22+X23+X24+X25+X26+X27+X28+X29+X30}
\StringTok{mfd =~ h123+f123+i123+a123+p123}

\StringTok{##fix the covariances}
\StringTok{harmL~~0*mfq}
\StringTok{fairL~~0*mfq}
\StringTok{ingroupL~~0*mfq}
\StringTok{authorityL~~0*mfq}
\StringTok{purityL~~0*mfq}
\StringTok{harmL~~0*mfd}
\StringTok{fairL~~0*mfd}
\StringTok{ingroupL~~0*mfd}
\StringTok{authorityL~~0*mfd}
\StringTok{purityL~~0*mfd}
\StringTok{authorityL~~.80*purityL}
\StringTok{mfq~~0*mfd}
\StringTok{'}

\NormalTok{model6.}\FloatTok{3.}\NormalTok{fit =}\StringTok{ }\KeywordTok{cfa}\NormalTok{(model6.}\DecValTok{3}\NormalTok{, }\DataTypeTok{data=}\NormalTok{mtmmdata, }\DataTypeTok{std.lv=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(model6.}\FloatTok{3.}\NormalTok{fit, }\DataTypeTok{rsquare=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{standardized=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{fitMeasures}\NormalTok{(model6.}\FloatTok{3.}\NormalTok{fit)}

\NormalTok{####fix the model####}
\NormalTok{model6.}\DecValTok{4}\NormalTok{ =}\StringTok{ '}
\StringTok{harmL =~ X1+X2+X4+X5+X6+h123}
\StringTok{fairL =~ X7+X8+X9+X10+X11+X12+f123}
\StringTok{ingroupL =~ X13+X14+X15+X17}
\StringTok{authorityL =~ X22+X23+X24+a123}
\StringTok{purityL=~ X25+X26+X27+X29+X30+p123}
\StringTok{mfq =~ X1+X2+X4+X5+X6+X7+X8+X9+X10+X11+X12+X13+X14+X15+X17+X22+X23+X24+X25+X26+X27+X29+X30}
\StringTok{mfd =~ h123+f123+a123+p123}

\StringTok{##fix the covariances}
\StringTok{harmL~~0*mfq}
\StringTok{fairL~~0*mfq}
\StringTok{ingroupL~~0*mfq}
\StringTok{authorityL~~0*mfq}
\StringTok{purityL~~0*mfq}
\StringTok{harmL~~0*mfd}
\StringTok{fairL~~0*mfd}
\StringTok{ingroupL~~0*mfd}
\StringTok{authorityL~~0*mfd}
\StringTok{purityL~~0*mfd}
\StringTok{authorityL~~.60*purityL}
\StringTok{h123~~2.44*h123}
\StringTok{f123~~1.85*f123}
\StringTok{'}

\NormalTok{model6.}\FloatTok{4.}\NormalTok{fit =}\StringTok{ }\KeywordTok{cfa}\NormalTok{(model6.}\DecValTok{4}\NormalTok{, }\DataTypeTok{data=}\NormalTok{mtmmdata, }\DataTypeTok{std.lv=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(model6.}\FloatTok{4.}\NormalTok{fit, }\DataTypeTok{rsquare=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{standardized=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{fitMeasures}\NormalTok{(model6.}\FloatTok{4.}\NormalTok{fit) }
\end{Highlighting}
\end{Shaded}

\subsection{Results}\label{results-2}

Data Cleaning and Descriptives. In sample 1, participants who wrote less
than 50 words were deleted (\emph{n} = 69) leaving \emph{n} = 221
participants. The average political orientation was 4.80 (\emph{SD} =
2.21) on a scale of 1 (\emph{conservative}) to 10 (\emph{liberal}). In
sample 2, all 160 participants wrote at least 50 words. The mean
political orientation was 5.01 (\emph{SD} = 2.33) for sample 2. The data
from sample 1 and sample 2 were combined. Before any analyses were
conducted, participants who did not use any words from the MFD were
deleted; 16 participants were deleted from sample 1 and 25 from sample
2. The final sample size for analysis was \emph{N} = 340 which had a
mean political orientation of 4.90 (\emph{SD} = 2.28). MFD scores were
computed using NVivo \emph{(CITE)} as both frequency for each foundation
and percent coverage for each foundation. Frequency was simply the count
of the number of words used from a given foundation dictionary; for
example, a participant using the word \emph{war} once and the word
\emph{peace} twice would have a frequency score of 3 for the \emph{harm}
dictionary. Percent coverage was calculated by taking the frequency and
dividing by the word count; for example, given a frequency score of 3
for the \emph{harm} dictionary and a word count of 100, then the percent
coverage would be .03 for the \emph{harm} dictionary. MFQ scores for
each foundation were calculated by averaging the six items pertaining to
each foundation. Reliability. Here we should talk about the reliability
of the MFQ for each piece, as well as the reliability of the words for
the MFD. I think to do that you might need a thing that has each word as
frequency count yes/no or however the LIWC version thing was done. MTMM.
BASIC SEM STUFF HERE (also that you used bayes) Data screening was
conducted using SPSS version 22 and AMOS version 22. Participants who
were missing data for the MFD, MFQ, or political orientation were
deleted from all analyses. Given the distribution of the dictionary
variables, participants whose writing sample were less than 2\% words
from the MFD were deleted resulting in a sample size of 252.
Additionally, 7 outliers were deleted. Widaman's (1985; as cited in
({\textbf{???}})) four-step nested method was used to test the
convergent and divergent validity of the MFD and MFQ. The first step is
the baseline model (Model 1), which establishes correlation among traits
(\emph{harm}, \emph{purity}, \emph{fairness}, \emph{authority}, and
\emph{ingroup}) as well as correlation among methods (MFD and MFQ) but
no cross correlation of traits and methods. The individual questions
from the 30 item version of the MFQ and the total frequency of concepts
from each foundation in the MFD were used as measured variables. The fit
of this first model indicated some misfit, as fit indices were a mix of
poor and acceptable, \(\chi\) 2 (514) = 977.46, \(\chi\) 2/df = 1.90,
CFI = .842, RMSEA = .061 {[}95\% CI = .055-.067{]}, SRMR = .0623. In
this model, the MFD \emph{harm}, \emph{fairness}, and \emph{ingroup}
items significantly loaded onto their trait factors, while
\emph{authority} and \emph{purity} did not. All foundations but
\emph{authority} loaded significantly on the method traits. All but two
of the \emph{ingroup} questions and one \emph{authority} question loaded
onto the MFQ trait factors. Several questions of the MFQ did not load
significantly onto the methods factors; however, this result was taken
as an indicator that traits variance was higher than methods variance.
Generally, trait loadings were higher than method loadings for both the
MFD and MFQ for \emph{harm} and \emph{fairness} traits. However, the
\emph{purity}, \emph{ingroup}, and \emph{authority} foundations did not
show this loading pattern. ERIN STOPPED HERE CUZ HEADACHE. The second
step (Model 2) involved eliminating the latent traits from the model.
This model was significantly worse than Model 1 indicating the traits
are important to the model (\(\delta\) \(\chi\) 2 = 1141.09, \(\delta\)
df = 45, \(\delta\) CFI = .351). This supports convergent validity for
the traits measured by both methods which in this case are the five
moral foundations. The third step (Model 3) involved forcing the five
traits to be perfectly correlated. This model was significantly worse
than Model 1 indicating the usefulness of five unique traits (\(\delta\)
\(\chi\) 2 = 311.09, \(\delta\) df = 10, \(\delta\) CFI = .097). This
supports discriminant validity for the existence of five unique moral
foundations. The final step (Model 4) involved allowing the correlations
between the traits to be freely estimated and forcing the methods to be
uncorrelated. This model was similar to Model 1 indicating the methods
both measure the traits but they are both unique methods (\(\delta\)
\(\chi\) 2 = 2.23, \(\delta\) df = 1, \(\delta\) CFI = .001). This
supports discriminant validity for the methods. This set of analyses
suggests the MFD is a possibly valid measure of moral foundations but
does not measure them well enough to be useful in all applications and
may be measuring them differently than the MFQ. Regression predicting
Political Orientation. The MFQ has predicted political orientation
across many studies (Federico et al., 2013; Graham et al., 2009; Haidt,
Graham, \& Joseph, 2009; Weber \& Federico, 2013). Therefore, in
addition to the MTMM analysis, we compared how well the MFD score
predicted political orientation compared to how well the MFQ predicted
the political orientation. First, total MFQ scores were calculated for
each foundation by averaging all six items. Then, a regression analysis
was conducted with the five MFQ foundation score predicting political
orientation. The overall model was significant, \emph{R2} = .35,
\emph{F} (5, 255) = 26.91, \emph{p} \textless{} .05. Higher scores on
the harm and fairness foundations predicted a more liberal political
orientation with harm accounting for 3\% of the variance and fairness
accounting for 6\%. Higher scores on ingroup, authority, and purity
predicted a more conservative orientation accounting for 1\%, 2\%, and
8\% on the variance respectively. See \emph{table ?} for regression
coefficients. Next, a regression analysis was conducted to determine how
well the five MFD scores predicted political orientation. The overall
model was not significant, \emph{R2} = .16, \emph{F} (5, 255) = 1.36,
\emph{p} = .241. Higher harm scores somewhat predicted more liberal
orientation accounting for 1\% of the variance in political orientation.
Higher purity scores somewhat predicted more conservative orientation
accounting for 1\% of the variance. See \emph{table ?} for regression
coefficients.

\subsection{Study 2}\label{study-2}

In Study 2, the MFD was applied to real-world data, U.S. Congressional
speeches. The purpose of this study was to further test the predictive
validity of the MFD. If valid, the MFD should detect political party
differences in congressional speeches.

\subsection{Method}\label{method-2}

\section{Sample}\label{sample}

Speeches were gathered through the Congressional Record available
through the U.S. Government Publishing Office. Speeches were gathered
from the following venues from 1998-2013: Senate, House of
Representatives, Senate Foreign Affairs Committee, and House Foreign
Affairs Committee. The topics of the speeches were U.S. foreign policy
with the following nations: Iraq, Iran, North Korea, Afghanistan,
Kosovo, Libya, Russia, Sudan, and Syria. These speeches often deal with
the use of military force and the enforcement of sanctions which should
include moral arguments. A total of 5207 Congressional speeches were
gathered. These speeches were made by 509 unique speakers. Republicans
gave 2268 speeches, and Democrats gave 2939 speeches. \# Data Processing
For each speech, the number of words used from each of the five
foundation dictionary was calculated. So, each speech had a word
frequency count for each foundation. Speeches which did not contain any
words from any foundation dictionary were excluded. Across speeches,
there were a total of 2,026,243 words. Of these, 7838 (.39\%) were
\emph{harm} words, 1976 (.10\%) were \emph{fairness} words, 2985 (.15\%)
were \emph{ingroup} words, 4057 (.20\%) were \emph{authority} words, and
717 (.04\%) were \emph{purity} words.

\subsection{Results}\label{results-3}

Bayesian \emph{t}-tests were used to compare the Democratic and
Republican use of MFD words. For \emph{harm} words, the Bayes factor
comparing a model of equal use between Democrats and Republicans and a
model of greater use by Democrats was .08. In other words, equal use of
\emph{harm} words by both parties is more likely. Examining the means
revealed that Democrats (\emph{M} = 5.62, \emph{SD} = 8.12), on average,
used less than one more \emph{harm} word than Republicans (\emph{M} =
4.87, \emph{SD} = 6.32). For \emph{fairness} words, the Bayes factor was
.04; once again, equal use of \emph{fairness} words by both parties is
more likely. Essentially no difference exists between the mean use for
Democrats (\emph{M} = 2.46, \emph{SD} = 2.74) and Republicans (\emph{M}
= 2.66, \emph{SD} = 3.34). For \emph{ingroup}, \emph{authority}, and
\emph{purity} words, a model of equal use was tested against a model of
greater use by Republicans. A Bayes factor of .10 supported greater
probability for the equal use of \emph{ingroup} words with little
difference between Republicans (\emph{M} = 2.55, \emph{SD} = 2.83) and
Democrats (\emph{M} = 2.48, \emph{SD} = 2.10). A Bayes factor of .04
also supported greater likelihood of the equal use of \emph{authority}
words with no substantial difference between Republicans (\emph{M} =
3.06, \emph{SD} = 3.43) and Democrats (\emph{M} = 3.22, \emph{SD} =
3.19). Likewise, a Bayes factor of .09 demonstrated a greater
probability for the equal use of \emph{purity} words with little to no
difference between Republicans (\emph{M} = 1.52, \emph{SD} = 1.03) and
Democrats (\emph{M} = 1.54, \emph{SD} = 1.04). See \emph{figure ?} for
all comparisons.

\subsection{Discussion}\label{discussion}

\begin{verbatim}
The preceding analyses seem to suggest the MFD has limited validity. While the step procedure of the MTMM supports the assumptions that the MFD is measuring the same constructs as the MFQ, the models themselves suggest the MFD measures moral foundations rather poorly. Furthermore, the MFD fails to have the predictive validity found in the MFQ predicting political orientation. Based on the initial work of [@Graham2009], it is possible that the MFD is measuring the moral foundation constructs differently than the MFQ as they did find ingroup words to predict liberal orientation rather than conservative. The problems with the MFD may be due to the use of word frequency. As we found in the current study, the words of the MFD are not used very often which forced us to delete many participants and also left many participants who used only 2 or 3 words from the dictionary. The infrequent use of the MFD words may have caused some mismeasurement of the foundations. The LSA approach taken by @Sagi2013 may be a solution to this problem and may represent a better application of the dictionary. A limitation of the current study was word count for the texts, which could have exacerbated the word frequency problem. The average word count in our study was around 100-200 words; increasing the word count may yield better results in future studies. However, overall it seems that the word frequency method of the MFD is a poor measure of moral foundations theory. Further research is required to either improve the MFD or determine what exactly it is measuring. 
\end{verbatim}

\hypertarget{refs}{}
\hypertarget{ref-Federico2013}{}
Federico, C. M., Weber, C. R., Ergun, D., \& Hunt, C. (2013). Mapping
the Connections between Politics and Morality: The Multiple
Sociopolitical Orientations Involved in Moral Intuition. \emph{Political
Psychology}, \emph{34}(4), 589--610.
doi:\href{https://doi.org/10.1111/pops.12006}{10.1111/pops.12006}

\hypertarget{ref-Graham2009}{}
Graham, J., Haidt, J., \& Nosek, B. A. (2009). Liberals and
conservatives rely on different sets of moral foundations. \emph{Journal
of Personality and Social Psychology}, \emph{96}(5), 1029--1046.
doi:\href{https://doi.org/10.1037/a0015141}{10.1037/a0015141}

\hypertarget{ref-Graham2012}{}
Graham, J., Nosek, B. A., \& Haidt, J. (2012). The Moral Stereotypes of
Liberals and Conservatives: Exaggeration of Differences across the
Political Spectrum. \emph{PLoS ONE}, \emph{7}(12), e50092.
doi:\href{https://doi.org/10.1371/journal.pone.0050092}{10.1371/journal.pone.0050092}

\hypertarget{ref-Haidt2004}{}
Haidt, J., \& Joseph, C. (2004). Intuitive ethics: how innately prepared
intuitions generate culturally variable virtues. \emph{Daedalus},
\emph{133}(4), 55--66.
doi:\href{https://doi.org/10.1162/0011526042365555}{10.1162/0011526042365555}

\hypertarget{ref-Haidt2009}{}
Haidt, J., Graham, J., \& Joseph, C. (2009). Above and Below
Left--Right: Ideological Narratives and Moral Foundations.
\emph{Psychological Inquiry}, \emph{20}(2-3), 110--119.
doi:\href{https://doi.org/10.1080/10478400903028573}{10.1080/10478400903028573}

\hypertarget{ref-Kertzer2014}{}
Kertzer, J. D., Powers, K. E., Rathbun, B. C., \& Iyer, R. (2014). Moral
Support: How Moral Values Shape Foreign Policy Attitudes. \emph{The
Journal of Politics}, \emph{76}(3), 825--840.
doi:\href{https://doi.org/10.1017/S0022381614000073}{10.1017/S0022381614000073}

\hypertarget{ref-Koleva2012}{}
Koleva, S. P., Graham, J., Iyer, R., Ditto, P. H., \& Haidt, J. (2012).
Tracing the threads: How five moral concerns (especially Purity) help
explain culture war attitudes. \emph{Journal of Research in
Personality}, \emph{46}(2), 184--194.
doi:\href{https://doi.org/10.1016/j.jrp.2012.01.006}{10.1016/j.jrp.2012.01.006}

\hypertarget{ref-Weber2013}{}
Weber, C. R., \& Federico, C. M. (2013). Moral Foundations and
Heterogeneity in Ideological Preferences. \emph{Political Psychology},
\emph{34}(1), 107--126.
doi:\href{https://doi.org/10.1111/j.1467-9221.2012.00922.x}{10.1111/j.1467-9221.2012.00922.x}






\end{document}
